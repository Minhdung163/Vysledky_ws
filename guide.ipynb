{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_publication_detail(tbody_xpath, driver, search_text):\n",
    "    # Find all rows in the tbody\n",
    "    rows = driver.find_elements(By.XPATH, f\"{tbody_xpath}/tr\")\n",
    "\n",
    "    # Initialize a variable to store the td text for the search_text\n",
    "    detail_td_text = None\n",
    "\n",
    "    # Iterate through each row\n",
    "    for row in rows:\n",
    "        # Find the th element in the current row\n",
    "        th = row.find_element(By.XPATH, \".//th\")\n",
    "        # Check if the th text matches the search_text\n",
    "        if th.text == search_text:\n",
    "            # If it does, find the td element in the same row and extract its text\n",
    "            td = row.find_element(By.XPATH, \".//td\")\n",
    "            detail_td_text = td.text\n",
    "            break  # Stop searching once found\n",
    "\n",
    "    return detail_td_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function is specially used for finding the place and year published of the publication. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_publication(driver,result_link):\n",
    "    driver.get(result_link)\n",
    "    sleep(2)\n",
    "\n",
    "    id = str(uuid4())\n",
    "    name = driver.find_element(By.XPATH, \"/html/body/div/main/div[1]/div[7]/div/div/div[2]/table/tbody/tr[3]/td\").text\n",
    "    published_year = find_publication_detail(\"/html/body/div/main/div[1]/div[7]/div/div/div[2]/table/tbody\", driver, \"Rok uplatnění\")\n",
    "    place_of_publication = find_publication_detail(\"/html/body/div/main/div[1]/div[8]/div/div/div[2]/table/tbody\", driver, \"Místo vydání\")\n",
    "    publication_type_name = driver.find_element(By.XPATH, \"/html/body/div/main/div[1]/div[7]/div/div/div[2]/table/tbody/tr[1]/td\").text\n",
    "    valid = True\n",
    "    \n",
    "    # Read the publication types from the JSON file\n",
    "    with open(\"publication_types.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "        data = json.load(f)\n",
    "    publication_types = data['publicationtypes']\n",
    "\n",
    "    # Find the publication type with the matching name and get its id\n",
    "    publication_type_id = next((item['id'] for item in publication_types if item['name'] == publication_type_name), None)\n",
    "\n",
    "    return {\n",
    "        \"id\": id, \n",
    "        \"name\": name, \n",
    "        \"published_year\": published_year, \n",
    "        \"place_of_publication\": place_of_publication, \n",
    "        \"publication_type_id\": publication_type_id, \n",
    "        \"valid\": valid, \n",
    "        \"reference\": result_link\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finding all variables of the publication, and also open the publication_types.json to match the type_id of the publication."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_publication_types(driver, result_link):\n",
    "    with open('publication_categories.json', 'r', encoding='utf-8') as f:\n",
    "        data = json.load(f)\n",
    "    \n",
    "    category_id = data['publicationcategories'][0]['id']\n",
    "    \n",
    "    driver.get(result_link)\n",
    "    sleep(2)\n",
    "\n",
    "    id = str(uuid4())\n",
    "    name = driver.find_element(By.XPATH, \"/html/body/div/main/div[1]/div[7]/div/div/div[2]/table/tbody/tr[1]/td\").text\n",
    "    \n",
    "    name_to_name_en_mapping = {\n",
    "        \"Uspořádání workshopu\": \"Workshop Organization\",\n",
    "        \"Ostatní\": \"Other\",\n",
    "        \"Audiovizuální tvorba\": \"Audiovisual Creation\",\n",
    "        \"Recenzovaný odborný článek\": \"Peer-reviewed Article\",\n",
    "        \"Stať ve sborníku\": \"Conference Proceedings Article\",\n",
    "        \"Kapitola/y v odborné knize\": \"Chapter/s in a Scholarly Book\",\n",
    "        \"Software\": \"Software\",\n",
    "        \"Odborná kniha\": \"Scholarly Book\",\n",
    "        \"Poloprovoz / technologie / odrůda / plemeno\": \"Semi-Operational / Technology / Variety / Breed\"\n",
    "    }\n",
    "    \n",
    "    name_en = name_to_name_en_mapping.get(name, \"Unknown\")\n",
    "    \n",
    "    return {\n",
    "        \"id\": id, \n",
    "        \"category_id\": category_id,\n",
    "        \"name\": name,\n",
    "        \"name_en\": name_en\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find all publication types and map czech name with english name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_list_of_authors(driver, result_link):\n",
    "    # Load the user.json file\n",
    "    with open(\"user.json\", \"r\", encoding='utf-8') as file:\n",
    "        users = json.load(file)\n",
    "        \n",
    "    # Load the publications.json file\n",
    "    with open(\"publications.json\", \"r\", encoding='utf-8') as file:\n",
    "        publications_data = json.load(file)\n",
    "    \n",
    "    driver.get(result_link)\n",
    "    sleep(2)\n",
    "    \n",
    "    try:\n",
    "        tbody_element = driver.find_element(By.XPATH, \"/html/body/div/main/div[1]/div[5]/div/div/div[2]/table/tbody\")\n",
    "        # Proceed with operations on tbody_element if found\n",
    "        rows = tbody_element.find_elements(By.TAG_NAME, \"tr\")\n",
    "        authors = []\n",
    "        \n",
    "        for row in rows:\n",
    "            cells = row.find_elements(By.TAG_NAME, \"td\")\n",
    "            author_name = cells[1].text\n",
    "            \n",
    "            # Search for the author in the loaded users\n",
    "            matching_user = next((user for user in users if user[\"name\"] == author_name), None)\n",
    "            \n",
    "            # If a matching user is found, use its ID; otherwise, generate a new UUID\n",
    "            user_id = matching_user[\"id\"] if matching_user else str(uuid4())\n",
    "            \n",
    "            # Search for a matching publication reference\n",
    "            matching_publication = next((pub for pub in publications_data[\"publications\"] if pub[\"reference\"] == result_link), None)\n",
    "            \n",
    "            # If a matching publication is found, use its ID for the author\n",
    "            if matching_publication:\n",
    "                publication_id = matching_publication[\"id\"]\n",
    "            \n",
    "            author = {\n",
    "                \"id\": str(uuid4()),\n",
    "                \"user_id\": user_id,\n",
    "                \"publication_id\": publication_id,\n",
    "                \"order\": cells[0].text,\n",
    "                \"share\": cells[2].text\n",
    "            }\n",
    "            authors.append(author)\n",
    "        return authors\n",
    "    except NoSuchElementException:\n",
    "        matching_publication = next((pub for pub in publications_data[\"publications\"] if pub[\"reference\"] == result_link), None)\n",
    "            \n",
    "        # If a matching publication is found, use its ID for the author\n",
    "        if matching_publication:\n",
    "            publication_id = matching_publication[\"id\"]\n",
    "        # Element not found, skip or handle accordingly\n",
    "        return [{\n",
    "            \"id\": str(uuid4()),  # Generate a new UUID for the id\n",
    "            \"user_id\": None,  # Explicitly set to None\n",
    "            \"publication_id\": publication_id, \n",
    "            \"order\": None,  # Explicitly set to None\n",
    "            \"share\": None  # Explicitly set to None\n",
    "        }]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the author of the publication based on the user name, which get from \"https://apl.unob.cz/vvi/Autori\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def login(url, username, password):\n",
    "    browser_options = webdriver.ChromeOptions()\n",
    "    browser_options.headless = True\n",
    "\n",
    "    driver = webdriver.Chrome(options=browser_options)\n",
    "    driver.get(url)  \n",
    "    \n",
    "    driver.refresh()\n",
    "    assert \"UoD\" in driver.title\n",
    "    \n",
    "    username_field = driver.find_element(By.NAME, \"Username\")\n",
    "    username_field.clear()\n",
    "    username_field.send_keys(username)\n",
    "    sleep(1)\n",
    "    \n",
    "    password_field = driver.find_element(By.NAME, \"Password\") \n",
    "    password_field.clear()\n",
    "    password_field.send_keys(password)\n",
    "    sleep(1)\n",
    "    \n",
    "    password_field.send_keys(Keys.RETURN)\n",
    "    \n",
    "    return driver"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Login to the web for scraping data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_publication_links(url, username, password):\n",
    "    driver = login(url, username, password)\n",
    "    while True:\n",
    "        try:\n",
    "            expand_button = driver.find_element(By.XPATH, \"/html/body/div/main/div[1]/div[3]/div/div[2]/button\")\n",
    "            actions = ActionChains(driver)\n",
    "            actions.move_to_element(expand_button).click(expand_button).perform()\n",
    "            sleep(2)\n",
    "        except NoSuchElementException:\n",
    "            break\n",
    "\n",
    "        # Check for the alert element\n",
    "        try:\n",
    "            alert_element = driver.find_element(By.CSS_SELECTOR, \".alert.m-0.alert-info\")\n",
    "            # If the element is found, break the loop\n",
    "            break\n",
    "        except NoSuchElementException:\n",
    "            # If the element is not found, continue with the next iteration\n",
    "            continue\n",
    "        \n",
    "    result_elems = driver.find_elements(By.CSS_SELECTOR, \"[href^='/vvi/Vysledek']\")\n",
    "    result_links = [result_elem.get_attribute(\"href\") for result_elem in result_elems]\n",
    "    result_links = set(result_links)\n",
    "    result_links = [link for link in result_links if 'Edit' not in link and 'Index' not in link]\n",
    "\n",
    "    driver.quit()\n",
    "\n",
    "    return result_links"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get all the publication links for easy extract data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_publication_user(url, username, password):\n",
    "    driver = login(url, username, password)\n",
    "    while True:\n",
    "        try:\n",
    "            expand_button = driver.find_element(By.XPATH, \"/html/body/div/main/div[1]/div[3]/div/div[2]/button\")\n",
    "            actions = ActionChains(driver)\n",
    "            actions.move_to_element(expand_button).click(expand_button).perform()\n",
    "            sleep(2)\n",
    "        except NoSuchElementException:\n",
    "            break\n",
    "\n",
    "        # Check for the alert element\n",
    "        try:\n",
    "            alert_element = driver.find_element(By.CSS_SELECTOR, \".alert.m-0.alert-info\")\n",
    "            # If the element is found, break the loop\n",
    "            break\n",
    "        except NoSuchElementException:\n",
    "            # If the element is not found, continue with the next iteration\n",
    "            continue\n",
    "    \n",
    "    tbody = driver.find_element(By.ID, \"AutoriListBody\")\n",
    "    rows = tbody.find_elements(By.TAG_NAME, \"tr\")\n",
    "\n",
    "    names = [row.find_element(By.TAG_NAME, \"td\").text for row in rows]\n",
    "    names = set(names)\n",
    "    \n",
    "    users = [{\"id\": str(uuid4()), \"name\": name} for name in names]\n",
    "\n",
    "    # Write the authors and UUIDs to a JSON file\n",
    "    with open(\"user.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(users, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "    driver.quit()\n",
    "\n",
    "    return users"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get all the users to easy match with the author of publication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_publication(url, username, password, result_links):\n",
    "    driver = login(url, username, password)\n",
    "    \n",
    "    publications = []  \n",
    "    for result_link in result_links:\n",
    "        publication_data = get_publication(driver, result_link)\n",
    "        publications.append(publication_data)  # Append each publication's data\n",
    "        \n",
    "    results = {\"publications\": publications}  # Wrap the accumulated list\n",
    "    return results\n",
    "\n",
    "def write_publication_types(url, username, password, result_links):\n",
    "    driver = login(url, username, password)\n",
    "    \n",
    "    publication_types_list = []\n",
    "    seen_names = set()\n",
    "    for result_link in result_links:\n",
    "        publication_types = get_publication_types(driver, result_link)\n",
    "        if publication_types['name'] not in seen_names:\n",
    "            seen_names.add(publication_types['name'])\n",
    "            publication_types_list.append(publication_types)\n",
    "    results = {\"publicationtypes\": publication_types_list}\n",
    "    return results\n",
    "\n",
    "def write_authors(url, result_links, username, password):\n",
    "    driver = login(url, username, password)\n",
    "    \n",
    "    authors = []\n",
    "    for result_link in result_links:\n",
    "        authors_data = get_list_of_authors(driver, result_link)\n",
    "        authors.append(authors_data)\n",
    "        \n",
    "    result = {\"publication_authors\": authors}\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These functions write each type of data to a separate json file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_publication_category():\n",
    "    new_id = str(uuid4())  # Generate a new UUID\n",
    "    category = {\n",
    "        \"publicationcategories\": [\n",
    "            {\n",
    "                \"id\": new_id,  # Use the generated UUID\n",
    "                \"name\": \"Vědecké\",\n",
    "                \"name_en\": \"Scientific\"  # Corrected typo from \"Scientic\" to \"Scientific\"\n",
    "            }\n",
    "        ]\n",
    "    }\n",
    "    \n",
    "    # Write the category dictionary to a JSON file\n",
    "    with open('publication_categories.json', 'w', encoding='utf-8') as f:\n",
    "        json.dump(category, f, ensure_ascii=False, indent=4)\n",
    "    \n",
    "    return category"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creat the category file for publication type. But just know one entity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def insert_externalidstypeid():\n",
    "    externalidtypes = []\n",
    "    new_type = {\n",
    "        \"id\": str(uuid4()),\n",
    "        \"name\": \"Vysledky\",\n",
    "        \"name_en\": \"Results\",\n",
    "        \"urlformat\": \"https://apl.unob.cz/vvi/Vysledky/%s\"\n",
    "    }\n",
    "    result = {\"externalidtypes\": new_type}\n",
    "    with open(\"externalidtypes.json\", \"w\") as type:\n",
    "        json.dump(result, type, indent=4)\n",
    "        \n",
    "def creat_externalids():\n",
    "    # Load the initial data from the JSON file\n",
    "    with open(\"publications.json\", \"r\", encoding='utf-8') as initial_file:\n",
    "        data = json.load(initial_file)\n",
    "        \n",
    "    with open(\"externalidtypes.json\", \"r\", encoding='utf-8') as f:\n",
    "        types = json.load(f)\n",
    "\n",
    "    # Create a new JSON structure for externalids\n",
    "    externalids = []\n",
    "\n",
    "    for publication in data[\"publications\"]:\n",
    "        externalid_types = types[\"externalidtypes\"]\n",
    "        outer_id = re.search(r'\\d+$', publication[\"reference\"]).group()\n",
    "        external_id_entry = {\n",
    "            \"id\": str(uuid4()),\n",
    "            \"inner_id\": publication[\"id\"],  \n",
    "            \"outer_id\": outer_id,\n",
    "            \"typeid_id\": externalid_types[\"id\"]\n",
    "        }\n",
    "        externalids.append(external_id_entry)\n",
    "\n",
    "    result = {\"externalids\": externalids}\n",
    "\n",
    "    # Save externalids to a JSON file\n",
    "    with open(\"externalids.json\", \"w\") as result_file:\n",
    "        json.dump(result, result_file, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creat externalids file for easy access of other project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_data():\n",
    "\n",
    "    with (open(\"publication_authors.json\", \"r\", encoding=\"utf-8\") as publication_authors,\n",
    "          open(\"publication_types.json\", \"r\", encoding=\"utf-8\") as publication_types,\n",
    "          open(\"publications.json\", \"r\", encoding=\"utf-8\") as publications,\n",
    "          open(\"externalids.json\", \"r\", encoding=\"utf-8\") as externalIds,\n",
    "          open(\"publication_categories.json\", \"r\", encoding=\"utf-8\") as publication_categories):\n",
    "\n",
    "        ext_ids = json.load(externalIds)\n",
    "        data_authors = json.load(publication_authors)\n",
    "        data_categories = json.load(publication_categories)\n",
    "        data_types = json.load(publication_types)\n",
    "        data_publication = json.load(publications)\n",
    "        \n",
    "        merged_data = [ext_ids, data_categories, data_types, data_authors, data_publication]\n",
    "\n",
    "        # Step 3: Write the merged data to a new JSON file\n",
    "        with open(\"systemdata.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(merged_data, f, ensure_ascii=False, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write all data to the systemdata.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def insert_publications_from_json(db_writer):\n",
    "    print(\"Starting to insert publications from JSON...\")\n",
    "\n",
    "    # Load publications data\n",
    "    with open(\"publications.json\", \"r\", encoding=\"utf-8\") as file:\n",
    "        publications = json.load(file)[\"publications\"]\n",
    "    print(f\"Loaded {len(publications)} publications.\")\n",
    "    print(publications[1])\n",
    "\n",
    "    # Load external IDs data\n",
    "    with open(\"externalids.json\", \"r\", encoding=\"utf-8\") as file:\n",
    "        externalids = json.load(file)[\"externalids\"]\n",
    "    print(f\"Loaded {len(externalids)} external IDs.\")\n",
    "\n",
    "    # Create a mapping from inner_id to external ID details for quick lookup\n",
    "    externalid_map = {item[\"inner_id\"]: item for item in externalids}\n",
    "    \n",
    "    awaitables = []\n",
    "    inserted_count = 0\n",
    "\n",
    "    for publication in publications:\n",
    "        try:\n",
    "            # Prepare data for insertion\n",
    "            table_name = \"Publication\"\n",
    "            publication_inner_id = publication.get(\"id\")\n",
    "\n",
    "            # Lookup the matching external ID entry using the map\n",
    "            matching_entry = externalid_map.get(publication_inner_id)\n",
    "\n",
    "            if matching_entry:\n",
    "                # Extract needed values if a matching entry is found\n",
    "                outer_id = matching_entry[\"outer_id\"]\n",
    "                outer_id_type_id = matching_entry[\"typeid_id\"]\n",
    "                # Insert with outer_id and outer_id_type_id\n",
    "                awaitables.append(db_writer.Create(table_name, publication, outer_id, outer_id_type_id))\n",
    "                print(f\"Inserted publication with inner_id {publication_inner_id} including external IDs.\")\n",
    "            else:\n",
    "                # Insert without outer_id and outer_id_type_id if no match is found\n",
    "                awaitables.append(db_writer.Create(table_name, publication))\n",
    "                print(f\"Inserted publication with inner_id {publication_inner_id} without external IDs.\")\n",
    "\n",
    "            if len(awaitables) > 9:\n",
    "                await asyncio.gather(*awaitables)\n",
    "                awaitables = []  # Reset the list after execution\n",
    "\n",
    "            inserted_count += 1\n",
    "        except Exception as e:\n",
    "            print(f\"Error inserting publication {publication_inner_id}: {e}\")\n",
    "\n",
    "    # Ensure any remaining awaitables are executed\n",
    "    if awaitables:\n",
    "        await asyncio.gather(*awaitables)\n",
    "        awaitables = []  # Reset the list after execution\n",
    "\n",
    "    print(f\"Finished inserting publications. Total attempted: {len(publications)}. Total successfully inserted: {inserted_count}.\")\n",
    "    return \"ok\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try to get the gql endpoit for publications, but still not working, maybe due to missing gql queryfile."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    #Logging in\n",
    "    with open(\"infor.txt\", \"r\") as f:\n",
    "        lines = f.readlines()\n",
    "    username = lines[0].strip()\n",
    "    password = lines[1].strip()\n",
    "    main_url = \"https://apl.unob.cz/vvi/Vysledky\"\n",
    "    base_url = \"https://apl.unob.cz/vvi/Vysledky?RokUplatneniList={year}&NazevVysledku=&Doi=&NazevCelku=&Issn=&KodUtIsi=&ScopusEid=&JeValidni=\"\n",
    "    author_url= \"https://apl.unob.cz/vvi/Autori\"\n",
    "    \n",
    "    # # Scrape the publication links\n",
    "    # for year in range(1951, 2025): \n",
    "    #     url = base_url.format(year=year)\n",
    "    #     links = scrape_publication_links(url, username, password)\n",
    "        \n",
    "    #     # Write the links to a text file\n",
    "    #     with open(f\"result_links_test.txt\", \"a\", encoding = \"utf-8\") as f:\n",
    "    #         for link in links:\n",
    "    #             f.write(link + \"\\n\")\n",
    "    \n",
    "    # insert_externalidstypeid()\n",
    "    \n",
    "    with open(\"result_links_test.txt\", \"r\") as file:\n",
    "        result_links = file.read().splitlines()\n",
    "    \n",
    "    # publication_data = write_publication(main_url, username, password, result_links)\n",
    "        \n",
    "    # with open(\"publications.json\", \"w\", encoding = \"utf-8\") as f:\n",
    "    #     json.dump(publication_data, f, ensure_ascii=False, indent=4, default=lambda o: '<not serializable>)')\n",
    "    \n",
    "    #creat_externalids()\n",
    "    \n",
    "    # publication_types_data = write_publication_types(main_url, username, password, result_links)\n",
    "    \n",
    "    # with open(\"publication_types.json\", \"w\", encoding = \"utf-8\") as f:\n",
    "    #     f.write(json.dumps(publication_types_data, ensure_ascii=False, indent=4))\n",
    "        \n",
    "    # # Scrape the publication users\n",
    "    # scrape_publication_user(author_url, username, password)    \n",
    "    \n",
    "    # author_data = write_authors(main_url, result_links, username, password)\n",
    "    \n",
    "    # with open(\"publication_authors2.json\", \"w\", encoding = \"utf-8\") as f:\n",
    "    #     f.write(json.dumps(author_data, ensure_ascii=False, indent=4))\n",
    "    \n",
    "    #merge_data()\n",
    "    \n",
    "    db_writer = DBWriter()  # Instantiate your DBWriter (adjust if the constructor requires parameters)\n",
    "    asyncio.run(insert_publications_from_json(db_writer))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Execute functions."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
